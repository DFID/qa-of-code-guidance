[
["index.html", "QA of Code Chapter 1 Introduction 1.1 Who is this guidance for? 1.2 What is its aim? 1.3 What does it cover? 1.4 Terminology", " QA of Code Joshua Halls 2019-07-30 Chapter 1 Introduction This guidance is published as part of the Quality guidance published by BPI in the ONS. This guidance has been created to support the Government Statistical Service. 1.1 Who is this guidance for? This guidance is for producers of official statistics who are using or want to use new method and techniques to improve and ensure that they use the best practice in the productrion of statistics. This is meant as an introduction to techiques and methods, not a compreheive learning resource. However, it is also not an introduction to coding, and you are likely to get more from this guidance if you are familar with coding principles, have used some script based language at some point, or currently use R/python for the production of statistics and are looking for how to increase the quality of your system. At the end of chapters there are links to resources which provide a more comprehive and indepth guide to the methods. These are not neccesarily the best or only learning resources. 1.2 What is its aim? There is an increasing move in the production of statistics from legacy systems to open source languages. This provides oppunities for using new tools and methods to enhance the quality of the systems used to produce statistics. Many producers of these systems are not data scientists but other analysts or non-analytical staff and may not be familar with best practice in software development. This guidance aims to introduce you to some of the major methods recommended to be implemented in a statistical output. 1.3 What does it cover? A number of departments have implemented code in their statistics to automate aspects of production. This may involve having a script whoch is run to achieve a part of production such as data valdiation or producing tables. 1.4 Terminology GIT SHELL VC Repository CLI GUI Unit Test "],
["rap.html", "Chapter 2 RAP 2.1 Why make our work reproducible? 2.2 What is a reproducible analytical pipeline?", " Chapter 2 RAP 2.1 Why make our work reproducible? Producing official statistics can be time-consuming and painstaking as we need to make sure that our outputs are both accurate and timely. Reproducible analysis is about opening up the production process so that anybody can follow the steps we took and understand how we got to the results that we publish. By making our analysis reproducible, we make it easier for others to quality assure, assess, critique and re-use our methods and results, and for colleagues to test and validate what we have done. In a reproducible workflow, we bring together the code and the data that we used to generate our results. This lets us be fully open about the decisions we made as we generated our outputs so that other analysts can follow what we did and re-create them. Reproducible analysis supports the requirements of the Code of Practice for Statistics around quality assurance and transparency, as, wherever possible, we share the code we used to build the outputs, along with enough data to allow for proper testing. 2.2 What is a reproducible analytical pipeline? Across government we aim to create effective and efficient statistical workflows which are repeatable over time and follow the principles of reproducible analysis. We call these Reproducible Analytically Pipelines (RAP). Producing statistics in a reproducible way is quite a new idea for government. Many analysts still use proprietary (paid-for) analytical tools like SAS or SPSS in combination with programs like Excel, Word or Acrobat to create statistical products. The processes for creating statistics this way are usually manual or semi-manual. Colleagues typically repeat parts of the process manually to quality assure the outputs. This way of working is time consuming and can be frustrating because the manual steps can be quite hard to replicate quickly. Work flows are also prone to error, because the input data and the outputs are not connected directly, only through the analyst’s manual intervention. More recently, the tools and techniques available to analysts have evolved. Open-source tools like Python and R, coupled with the use of version control tools such as Git, used widely in software engineering to make it easier to collaborate, have made it possible to develop more automatic, streamlined processes, accompanied by a full audit trail. RAP was first piloted in the GSS in 2017 by analysts in the Department for Digital, Culture, Media &amp; Sport (DCMS) and the Department for Education (DfE). They collaborated with data scientists from the Government Digital Service (GDS) to automate the production of statistical bulletins. To support the adoption of RAP across government, there is a network of RAP champions. Champions are responsible for promoting reproducible analysis and the use of reproducible analytical pipelines and supporting others who want to develop RAP in their own teams. More information on "],
["levels-of-rap.html", "Chapter 3 Levels of RAP", " Chapter 3 Levels of RAP RAP can be viewed as a process, starting with a basic coding project and adding additional levels of enhancement to the project as you go. This process gives a framework for progressing and how far you get depends on the skill level within your team, the infrastructure available in your department and even how necessary the later steps are for your project. For one off pieces of work then the later steps might be time consuming with minimal benefit. However for regular work you are going to repeat then the more levels you can add the more secure you will be that your code is preforming as you expect it to. This process should be pragmatic and proportionate. You may think about a minimal level that you hope to achieve before the code is put into production, but this level will vary depending on your needs. This piece of guidance will go through many of the steps. "],
["testing.html", "Chapter 4 Testing", " Chapter 4 Testing When you are working on a project how do you know that your code is doing what you expect it to do? You run it and look at the output. This is a manual test and it has it’s limitations. Hadley Wickham in The R Journal it’s not that we don’t test our code, it’s that we don’t store our tests so they can be re-run automatically. Manual tests need to be constantly re-run. This is achievable when your code is small but when it becomes longer this is time consuming. When you come back to your code in the future you will have forgotten what tests you have run, any changes are then liable to break something. Fundamentally if you don’t test your code then how do you know that your code is doing what it is suppose to? Just as data validation in an important step in statistics to the the data is correct, code validation with testing is important to check that the process is correct. 4.0.1 Advantages of testing your code Better Code Structure - Writing tests forces you to separate code into functions that are separate and isolated, reducing duplication in your code. This approach will make your code easier to test and also easier to understand and use. Less Bugs - Like double entry book keeping the behavior of your code is specified twice in the code and the test making it less error prone. More Robust Code - If all the major functions of your code have associated tests then you can change your code without worrying that you have broken something without realizing Decreased Frustration - Untested code is precarious, an making changes without knowing if you have broken a section is stress inducing. Being able to deploy code that you are confident will work because it is tested will make the process more productive. 4.0.2 When should you use testing In a Analytics pipeline. In general if you are going to reuse your code multiple times then it needs to be tested to ensure that it is working properly. In a Important piece of work such as a National Statistic. It is important that National Statistics are correct and that the public has confidence in the validity of published work, testing is important to ensure that the code is working as expected and prevents errors building confidence. When working in a team. Testing ensures that collaborators can alter the code without worrying about breaking the code without realizing. 4.0.3 When should you not use testing One off pieces of work, the effort may not be worth the benefit Exploratory work. If you are experimenting and are not sure whether your work will be converted into permanent project then testing may not be appropriate. There would be a benefit to testing if a exploratory project turns into a permanent one, however the code may be changed so fundamentally that all the tests would have to be re written. Fundamentally it is a trade off. "],
["testing-frameworks.html", "Chapter 5 Testing Frameworks 5.1 What excatly is a test 5.2 Refernces", " Chapter 5 Testing Frameworks 5.1 What excatly is a test Testing is just checking that the expected result is the same as the actual result. Automated testing is moving from doing this in a informal ad-hoc way to an automated process which is repeatable. Tests should be Fast Independent Repeatable (deterministic) Self-validating (no manual steps) Thorough (How much do you trust they cover everything?) 5.1.1 Unit Test 5.1.2 Intergration Tests 5.1.3 Test Driven Development 5.2 Refernces Testing Blog Testthat "],
["git.html", "Chapter 6 GIT 6.1 Git Introduction 6.2 About Version Control 6.3 What is GIT 6.4 How does GIT actually work 6.5 Typical Git workflow 6.6 Remote Repostory 6.7 Branching 6.8 Pull Request and issues", " Chapter 6 GIT What is GIT What is VC How to use GIT ( CLI or GUI) Typical GIT workflow Branching 6.1 Git Introduction Git is a version control system, a tool that tracks changes to your code and shares those changes with others. Git is most useful when combined with GitHub, a website that allows you to share your code with the world, solicit improvements via pull requests and track issues. 6.2 About Version Control Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This means that changes can be examined allowing a record to kept of who changed what. The changes can also be reversed, allowing your code to be rolled back to an earlier state. GIT is an immensely popular tool in the data science community for VC. 6.3 What is GIT Git is a version control system, a tool that tracks changes to your code and shares those changes with others. To start a common mistake is to get Git and it’s hosting services mixed up, Git is not GitHub or GitLab, they are websites for hosting projects using git. Git is a Distributed VC, meaning that you will have a local repository which has a special folder inside called .git. You will normally, but don’t have too, have a remote, central repository where you and collaborators can contribute code, this could be on GitHub, GitLab or elsewhere. You and any collaborators have an exact clone of the repository on your local computer. 6.4 How does GIT actually work You are probably familiar with the windows file system. Imagine a folder with some files/data in it. We can make the folder into a Git repository. When we do this Git will keep track of any changes you make to the files, and therefore allow you to see who has changed the files, work with other people on the files and undo any changes. Git thinks of the data as a series of snapshots of a mini file system (the folder). At any time you can take a snapshot, by saving or committing, and git will take a snapshot of what your files look like at that moment, and stores a reference to that snapshot. Git thinks about the data as a stream of snapshots. To actually run Git there are two methods CLI Command Line Interface GUI Graphical User Interface A GUI is effectively a program that will run git for you. However these only implement a subset of git commands and it is generally recommend to run Git on the command line. To do this you need to get a terminal and Git installed. This is easier than it sounds. A popular method is to get GitBash. This is a terminal interface with Git installed. If you are unfamiliar with running terminal commands there are alot of great online resources, and fundamentally you will only need a handful of commands to run what you need. If you don;t want to learn past that you won’t need to. 6.5 Typical Git workflow There are many ways to get a Git repository started. I will discuss turning your folder into a repository. First we need to initialize Git, we can do that with the command git init All git command start with git followed by the command. This will set-up the folder as a git repository, easy! Git won;t actually track any of the files yet, as we haven’t got any snapshots. We do this in two steps. First we add the files we want to be tracked to stage them to be committed. git add . This uses the command add to select the files we want to prepare to track, and . selects all files in the folder. Only do this if you want to track every file! Otherwise replace . with the filename you want such as git add data.csv Now the files are staged for the next commit, we want to take the snapshot. To commit we run git commit -m &quot;a helpful commit comment, anything you want!&quot; This runs the commit command which we take the snapshot. -m tells the commit that we want to leave a message and everything in \"\" is attached as a commit comment. Make this something informative and helpful, as you will want to later look back at these, and others will see them too. That is the basic git workflow for working by yourself. You can then edit the files however you want, e.g in Microsoft word or using R studio, and when you next want to take a snapshot run the command again git add . git commit -m &quot;another informative comment&quot; We can also check on the status of our repository with git status 6.6 Remote Repostory Using Git just on your computer is useful for being able to roll back or reverse to previous commits, and for keeping track of what changes you made. But Git is really powerful when combined with a remote repository. This basically creates a clone or identical copy of your git repository (all the files in it) and hosts it on another computer, probably on a website such as GitHub or Gitlab. This allows others to also make a copy of the code, to modify and share there changes with you whilst git keeps track. This is a useful tool for peer review and collaborator coding. it is also a backup if your laptop dies or is lost, as the data is safely stored elsewhere too. To do this is simple. First you need to setup a repository on a hosting site. Follow the instructions on GitHub or elsewhere on how to do this, it is not difficult. However make it an empty repository, no README file. Next we need to connect our git repository to the remote repository. Open a Shell and run: git remote add origin https://github.com/username/reponame This command is a bit more complex but you will only need to run it once. remote add is telling git to add a remote repository to the current git. origin will be the name of the remote repository. https://github.com/username/reponame is the URL of the repository. You will be able to on GitHub or gitlab copy and paste this into the terminal window. Now we have a remote repository added we can push our code to it. The typical workflow is the same but with an added step. First we add our files to be staged, we then commit them taking a snapshot of what the current folder looks like, and then we push this snapshot to the remote repository. git add . git commit -m &quot;helpful and informative commit comment&quot; git push Now if you go to the website you will be able to see your code. If a collaborator now wants to get an exact copy of code they can clone the repository. The folder will be copied onto their computer. git clone https://github.com/username/reponame They run this command on their computer and they will now have the files and you can both work together by following the regular workflow. At this stage there will be your repository, a remote repository on a website and a third repository on a collaborator laptop. 6.7 Branching Branching and merging is what makes Git so powerful and for what it has been optimized for, being a distributed version control system (VCS). Branching is one of the key features of Git. It allows you to diverge from the main lien of development. You might want to do this to run an experiment which you are not sure will work, such as rewriting a section of the code, which you will incorporate only if it succeeds. Yo may create a branch for new features, which are siloed until they are ready to be incorporated. Branching allows you to control the layout of your project and it is a useful tool for working collaboratively as a project can be broken into sections which are worked on branches and incorporated when ready. You can switch back and forth to the original “master branch” or to any other branch whenever is needed. When you work using Git you will always be on a branch. by default when you setup Git it automatically creates a “master” branch. If you want to develop a new feature or section you can easily create a new branch: git branch new_branch_name git checkout new_branch_name branch followed by a name will create a branch with that name and checkout followed by a name of an existing branch will switch to it. If you want to fold the code on a branch into another to combine the project onto branch this can be done by merging. To fold the code on the new branch into he master branch then you need to move onto the master branch and then merge. git checkout master git merge new_branch_name 6.8 Pull Request and issues Pull requests let you tell others about changes you’ve pushed to a branch in a repository on GitHub or GitLab. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. This is a powerful tool for managing a project and for peer review of code. Team members can work on code on a branch and when they are done open a pull request to merge the branch into the main master branch. Another team member will review this code and can either agree to merge, or post comments on potential improvements. This back and forth ensures errors are found early and in general improves the quality of the project code. This process ensures on high quality code makes it into production. Issues are another method of discussing your code and are a great way to keep track of tasks, bugs or enchantments. For example if there is a section of messy, hard to read code then a issue could be left to rewrite that section. The issue could be general or assigned to an individual with a due date. This is another useful method for managing a coding project. "],
["intro.html", "Chapter 7 intro", " Chapter 7 intro "],
["coding-guidelines.html", "Chapter 8 Coding Guidelines 8.1 Why Useful 8.2 Best Practice: Coding Principles 8.3 Style Guides", " Chapter 8 Coding Guidelines Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. – The Tidyverse Style Guide by Hadley Wickham Best practices are more about what code you write than how you write it. For example, there are many ways to access databases, but one particular way might be considered ‘best practice’ so that everyone in a team knows how it works and doesn’t’t have to learn other methods. 8.1 Why Useful Coding guidelines are helpful Peer Review Makes it easier to learn Often people new to coding want soem guidelines as this makes it easier to learn. It is hard to know what is best practice, some guidelines can be helpful. Makes maintaining code easier Just as important as writing the code is maintaining in production, perhaps after the writers have left. Maintainability is always an important consideration to make projects sustainable. Makes projects more coherent If code is written by a team then if everyone is writing in unique distinct styles then the project will be harder to read when combined. 8.1.1 Peer Review An important element of getting quality code is peer review. Showing people your code and getting it scrutinized is an important part of ensuring that code is doing what it is supposed to and allows iteration, a process where the back and forth of reviewing code cleans and sharpens it. This process is made easier by coding standards. When code is written in a familiar style it will be easier to quickly pick it up and understand what was your intention. Poorly written code may achieve the desired output but it may be difficult for others to read leading to code that only you have seen which may contain errors or have unforeseen side effects. Fundamentally using the same language is the first step to ensuring peer review. A business area which uses excel, R, SAS, SQL and python will struggle to ensure peer review. It is desirable for others outside your team to review your code, but this is often challenging depending on systems and data sensitivity. However code is often not sensitive, as it only details what you do to the data which will probably be disclosed anyway. Using dummy data or synthetic data which has the same structure and patterns as the actual raw data but has been randomly generated can allow peer review from those who do not have access to the data. 8.2 Best Practice: Coding Principles Specific and prescription coding guidelines ensure that all the code follows a similar style but can be difficult to implement. However there are some principles which apply to all languages and can be used to improve the quality of your code. Naming The names of variables &amp; functions need to be descriptive (intention-revealing), pronounceable and need to reflect what a function does and what the variables are. Short vs Readability Being efficient with writing does not always lead to a code that is readable, don’t sacrifice readability to make your code shorter. Distinct (Ubiquitous language) Make meaningful distinctions. Do not use the same name for two (or more) distinct items. Use the same term in the code, in the documents and when talking to customers. eg Do not use the cancer name for the Data and the model. Use cancerData and cancerModel for the Data and model respectively. The same is true for Dataframes and columns of the Dataframes. Code can be self-documenting. Put the effort to improve, organize and structure your code i.e. clean it. Keep functions simple Keep as few arguments as possible when creating a function. A function should aim to do one thing well, if there are lots of arguments then maybe that is a sign you should think about separating the function into multiple functions. The idea it to keep the code together that can get damaged for the same reason of change, when changing something you want to impact as few things as possible. Nesting. You should always avoid nesting as it make the code harder to read and understand. 8.3 Style Guides There are many style guides out there. You may choose to adopt one or part of one. Your team or department could also write some guidelines tailored to your needs to encourage best pratice. Here are some existing guidelines: Best Practice in Programming for Data Scientists: Google’s R Style Guide TidyVerse Style Guide The State of Naming Conventions in R PEP 8 – Style Guide for Python Code Ministry of Justice Analytical Services Coding Standards Data Science Campus Coding Standards Health and Social Care Scotland R Resources "]
]
